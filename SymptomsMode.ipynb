{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72960817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['HF_HOME'] = str(pathlib.Path(\"~/scratch-llm/storage/cache/huggingface/\").expanduser().absolute()) # '/scratch-llm/storage/cache/'\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"~/scratch-llm/storage/models/\"\n",
    "\n",
    "import torch, pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from nebulagraph_lite import nebulagraph_let as ng_let\n",
    "from llama_index.graph_stores.nebula import NebulaPropertyGraphStore\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.vector_stores.simple import SimpleVectorStoreData, SimpleVectorStore, VectorStoreQuery\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from typing import List\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c152bfb",
   "metadata": {},
   "source": [
    "# NebulaGraph conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NebulaGraph JupyterNotebook extension\n",
    "# !udocker pull vesoft/nebula-metad:v3\n",
    "# !udocker create --name=nebula-metad vesoft/nebula-metad:v3\n",
    "# !udocker setup --execmode=F1 nebula-metad\n",
    "# !udocker pull vesoft/nebula-graphd:v3\n",
    "# !udocker create --name=nebula-graphd vesoft/nebula-graphd:v3\n",
    "# !udocker setup --execmode=F1 nebula-graphd\n",
    "# !udocker pull vesoft/nebula-storaged:v3\n",
    "# !udocker create --name=nebula-storaged vesoft/nebula-storaged:v3\n",
    "# !udocker setup --execmode=F1 nebula-storaged\n",
    "\n",
    "\n",
    "n = ng_let(in_container=True)\n",
    "n.start() # Takes around 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext ngql\n",
    "%ngql --address 127.0.0.1 --port 9669 --user root --password nebula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb28b1",
   "metadata": {},
   "source": [
    "# Vector + Graph store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb5e0c",
   "metadata": {},
   "source": [
    "## SimpleVectorStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual data into all_nodes_embeddded\n",
    "with open(os.path.expanduser('~/scratch-llm/storage/nodes/all_nodes_all-mpnet-base-v2.pkl'), 'rb') as f:\n",
    "    all_nodes_embedded: List[TextNode] = pickle.load(f)\n",
    "# Create dictionaries from the nodes\n",
    "embedding_dict = {node.id_: node.get_embedding() for node in all_nodes_embedded}\n",
    "text_id_to_ref_doc_id = {node.id_: node.ref_doc_id or \"None\" for node in all_nodes_embedded}\n",
    "metadata_dict = {node.id_: node.metadata for node in all_nodes_embedded}\n",
    "\n",
    "# Initialize the SimpleVectorStore with the dictionaries\n",
    "vector_store = SimpleVectorStore(\n",
    "    data = SimpleVectorStoreData(\n",
    "        embedding_dict=embedding_dict,\n",
    "        text_id_to_ref_doc_id=text_id_to_ref_doc_id,\n",
    "        metadata_dict=metadata_dict,\n",
    "    ),\n",
    "    stores_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716316ff",
   "metadata": {},
   "source": [
    "## NebulaPropertyGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = NebulaPropertyGraphStore(\n",
    "    space = \"PrimeKG\",\n",
    "    username = \"root\",\n",
    "    password = \"nebula\",\n",
    "    url = \"nebula://localhost:9669\",\n",
    "    props_schema= \"\"\"`node_index` STRING, `node_type` STRING, `node_id` STRING, `node_name` STRING, \n",
    "        `node_source` STRING, `mondo_id` STRING, `mondo_name` STRING, `group_id_bert` STRING, \n",
    "        `group_name_bert` STRING, `orphanet_prevalence` STRING, `display_relation` STRING \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15941133",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc99d8",
   "metadata": {},
   "source": [
    "## Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ad96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", padding_side=\"left\", device_map=\"auto\")    \n",
    "if tokenizer.pad_token_id is None: #no <pad> token previously defined, only eos_token\n",
    "    tokenizer.pad_token = \"<|end_of_text|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    context_window=8192,\n",
    "    max_new_tokens=3048,\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.10, \n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"top_k\": 10, \n",
    "        \"top_p\": 0.9,\n",
    "        # \"repetition_penalty\": 0.9,  # Added to reduce repetition\n",
    "        # \"no_repeat_ngram_size\": 3,  # Prevents repetition of n-grams\n",
    "    },\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    "    # device_map=\"auto\",  # Automatically offload layers to CPU if GPU memory is insufficient\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    stopping_ids=[tokenizer.eos_token_id],\n",
    "    tokenizer_kwargs={\"max_length\": None},\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # BAAI/bge-small-en-v1.5 /  m3 / sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f00da0",
   "metadata": {},
   "source": [
    "# SymptomsMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataFilters, FilterOperator\n",
    "\n",
    "# Example usage\n",
    "phenotype_dict = {\n",
    "    \"key\": \"node_type\",\n",
    "    \"value\": \"effect/phenotype\",\n",
    "    \"operator\": FilterOperator.EQ\n",
    "}\n",
    "\n",
    "phenotype_filter = MetadataFilters(filters=[phenotype_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymptomsMode():\n",
    "    def __init__(self, vector_store: SimpleVectorStore, graph_store: NebulaPropertyGraphStore):\n",
    "        self.vector_store = vector_store\n",
    "        self.graph_store = graph_store\n",
    "\n",
    "    def retrieve(self, query: List[str]):\n",
    "        if not isinstance(query, list):\n",
    "            return None\n",
    "            \n",
    "        from numpy import dot\n",
    "        from numpy.linalg import norm\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        disease_counter = {}\n",
    "        total_symptoms = len(query)\n",
    "        match_threshold = int(total_symptoms * 0.8)\n",
    "        print(f\"Processing {total_symptoms} symptoms: {query}\")\n",
    "        \n",
    "        # Pre-compute all symptom embeddings\n",
    "        symptom_embeddings = {s: Settings.embed_model.get_text_embedding(s) for s in query}\n",
    "        \n",
    "        # First pass - collect all diseases with their embeddings\n",
    "        for symptom in query:\n",
    "            query_embedding = symptom_embeddings[symptom]\n",
    "            vector_store_query = VectorStoreQuery(\n",
    "                query_embedding=query_embedding,\n",
    "                similarity_top_k=1,\n",
    "                filters=phenotype_filter,\n",
    "            )\n",
    "            individual_results = vector_store.query(vector_store_query)\n",
    "            individual_results = zip(individual_results.ids, individual_results.similarities)\n",
    "\n",
    "            for node_id, score in individual_results:\n",
    "                kg_node = graph_store.get(ids=[node_id])[0]\n",
    "                \n",
    "                # Get related diseases from graph\n",
    "                graph_nodes = graph_store.structured_query(\n",
    "                    \"\"\"\n",
    "                    MATCH (e:Node__) WHERE id(e) == $ids\n",
    "                    MATCH p=(e)-[r:Relation__{label:\"disease-phenotype-positive\"}]-(t) \n",
    "                    RETURN DISTINCT id(t), t.Props__.node_name, t.Chunk__.text\n",
    "                    \"\"\", \n",
    "                    param_map={\"ids\": node_id}\n",
    "                )\n",
    "                \n",
    "                # Process each related disease\n",
    "                for node in graph_nodes:\n",
    "                    disease_id = node['id(t)']\n",
    "                    disease_name = node['t.Props__.node_name']\n",
    "                    node_text = f\"{disease_name}: {node.get('t.Chunk__.text', '')}\"\n",
    "                    \n",
    "                    # Add or update disease in counter\n",
    "                    if disease_id not in disease_counter:\n",
    "                        disease_counter[disease_id] = {\n",
    "                            'index': disease_id,\n",
    "                            'name': disease_name,\n",
    "                            'embedding': Settings.embed_model.get_text_embedding(node_text),\n",
    "                            'count': 1,\n",
    "                            'symptoms': [symptom],\n",
    "                            'cross_similarities': {}\n",
    "                        }\n",
    "                    else:\n",
    "                        disease_counter[disease_id]['count'] += 1\n",
    "                        if symptom not in disease_counter[disease_id]['symptoms']:\n",
    "                            disease_counter[disease_id]['symptoms'].append(symptom)\n",
    "        \n",
    "        # Second pass - calculate cross-similarities\n",
    "        for data in disease_counter.values():\n",
    "            # Calculate cross-similarity for each unmatched symptom\n",
    "            unmatched_symptoms = [s for s in query if s not in data['symptoms']]\n",
    "            \n",
    "            if unmatched_symptoms:\n",
    "                similarities = {\n",
    "                    symptom: dot(data['embedding'], symptom_embeddings[symptom]) / \n",
    "                             (norm(data['embedding']) * norm(symptom_embeddings[symptom]))\n",
    "                    for symptom in unmatched_symptoms\n",
    "                }\n",
    "                data['cross_similarities'] = similarities\n",
    "                data['avg_cross_similarity'] = sum(similarities.values()) / len(similarities)\n",
    "            else:\n",
    "                data['avg_cross_similarity'] = 0  # No unmatched symptoms\n",
    "                data['cross_similarities'] = {}\n",
    "                \n",
    "            # Add the number of matched symptoms to the similarity score\n",
    "            data['combined_score'] = data['count'] + data['avg_cross_similarity']\n",
    "        \n",
    "        # Group diseases by count\n",
    "        count_groups = defaultdict(list)\n",
    "        for disease_id, data in disease_counter.items():\n",
    "            count_groups[data['count']].append((disease_id, data))\n",
    "        \n",
    "        # Sort each group by combined score\n",
    "        for count, disease_list in count_groups.items():\n",
    "            count_groups[count] = sorted(disease_list, key=lambda x: x[1]['combined_score'], reverse=True)\n",
    "        \n",
    "        # Display results by count groups in descending order\n",
    "        count_keys = sorted(count_groups.keys(), reverse=True)\n",
    "        \n",
    "        print(\"\\n=== Diseases grouped by symptom match count ===\")\n",
    "        for count in count_keys:\n",
    "            print(f\"\\n--- Group: {count}/{total_symptoms} symptoms matched ---\")\n",
    "            for disease_id, data in count_groups[count]:\n",
    "                # Display combined score (count + avg_cross_similarity)\n",
    "                print(f\"ID: {data['index']} | Disease: {data['name']} | \"\n",
    "                      f\"Avg cross-similarity: {data['combined_score']:.5f}\")\n",
    "                \n",
    "                # Show cross-similarities for individual unmatched symptoms\n",
    "                if data['cross_similarities']:\n",
    "                    for symptom, score in data['cross_similarities'].items():\n",
    "                        print(f\"    - {symptom}: {score:.5f}\")\n",
    "                    print('\\n')\n",
    "        \n",
    "        return {\n",
    "            'count_groups': {k: v for k, v in count_groups.items()},\n",
    "            'all_diseases': disease_counter\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9592c",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = [\"Absence of subcutaneous fat\",\n",
    "\"Generalized abnormality of skin\",\n",
    "\"Micrognathia\",\n",
    "\"Narrow mouth\",\n",
    "\"Premature skin wrinkling\",\n",
    "] # Hutchinson-Gilford progeria syndrome\n",
    "\n",
    "# [\"Breast carcinoma\",\n",
    "#       \"Neoplasm of the pancreas\",\n",
    "#       \"Ovarian neoplasm\",\n",
    "#       \"Abnormal fallopian tube morphology\",\n",
    "#       \"Prostate cancer\",\n",
    "# ] # Hereditary breast and ovarian cancer syndrome\n",
    "\n",
    "\n",
    "context = SymptomsMode(vector_store, graph_store).retrieve(user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf01a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
