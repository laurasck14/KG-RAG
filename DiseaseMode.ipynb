{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os, json\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['HF_HOME'] = str(pathlib.Path(\"~/scratch-llm/storage/cache/huggingface/\").expanduser().absolute()) # '/scratch-llm/storage/cache/'\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"~/scratch-llm/storage/models/\"\n",
    "\n",
    "import torch, pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from nebulagraph_lite import nebulagraph_let as ng_let\n",
    "from llama_index.graph_stores.nebula import NebulaPropertyGraphStore\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.vector_stores.simple import SimpleVectorStoreData, SimpleVectorStore, VectorStoreQuery\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from typing import List\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NebulaGraph conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;47;75;124mMessage: Activating storaged...\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145mResult of `SHOW HOSTS`:\u001b[0m\n",
      "\u001b[1;3;38;2;47;75;124m    errors:\u001b[0m\n",
      "\u001b[1;3;38;2;47;75;124m        code: 0\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145m    results:\u001b[0m\n",
      "\u001b[1;3;38;2;47;75;124m        spaceName: \u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145m        data:\u001b[0m\n",
      "\u001b[1;3;38;2;47;75;124m            meta:\u001b[0m\n",
      "\u001b[1;3;38;2;47;75;124m                None, None, None, None, None, None, None\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145m            row:\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145m                127.0.0.1, 9779, ONLINE, 201, PrimeKG:100, PrimeKG_nebula:100, basketballplayer:1, PrimeKG:100, PrimeKG_nebula:100, basketballplayer:1, 3.8.0\u001b[0m\n",
      "\u001b[1;3;38;2;160;81;149m        columns:\u001b[0m\n",
      "\u001b[1;3;38;2;160;81;149m            Host, Port, Status, Leader count, Leader distribution, Partition distribution, Version\u001b[0m\n",
      "\u001b[1;3;38;2;212;80;135m        errors:\u001b[0m\n",
      "\u001b[1;3;38;2;47;75;124m            code: 0\u001b[0m\n",
      "\u001b[1;3;38;2;249;93;106m        latencyInUs: 1464\u001b[0m\n",
      "\u001b[1;3;38;2;168;255;159mInfo: loading basketballplayer dataset...\u001b[0m\n",
      "\u001b[1;3;38;2;210;161;255m\n",
      "  _   _      _           _        ____                 _     \n",
      " | \\ | | ___| |__  _   _| | __ _ / ___|_ __ __ _ _ __ | |__  \n",
      " |  \\| |/ _ | '_ \\| | | | |/ _` | |  _| '__/ _` | '_ \\| '_ \\ \n",
      " | |\\  |  __| |_) | |_| | | (_| | |_| | | | (_| | |_) | | | |\n",
      " |_| \\_|\\___|_.__/ \\__,_|_|\\__,_|\\____|_|  \\__,_| .__/|_| |_|\n",
      "                                                |_|          \n",
      "                                                lite version\n",
      "\u001b[0m\n",
      "\u001b[1;3;38;2;210;161;255m[ OK ] nebulagraph_lite started successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load NebulaGraph JupyterNotebook extension\n",
    "# !udocker pull vesoft/nebula-metad:v3\n",
    "# !udocker create --name=nebula-metad vesoft/nebula-metad:v3\n",
    "# !udocker setup --execmode=F1 nebula-metad\n",
    "# !udocker pull vesoft/nebula-graphd:v3\n",
    "# !udocker create --name=nebula-graphd vesoft/nebula-graphd:v3\n",
    "# !udocker setup --execmode=F1 nebula-graphd\n",
    "# !udocker pull vesoft/nebula-storaged:v3\n",
    "# !udocker create --name=nebula-storaged vesoft/nebula-storaged:v3\n",
    "# !udocker setup --execmode=F1 nebula-storaged\n",
    "\n",
    "n = ng_let(in_container=True)\n",
    "n.start() # Takes around 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;0;135;107m[OK] Connection Pool Created\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PrimeKG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PrimeKG_nebula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>basketballplayer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name\n",
       "0           PrimeKG\n",
       "1    PrimeKG_nebula\n",
       "2  basketballplayer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext ngql\n",
    "%ngql --address 127.0.0.1 --port 9669 --user root --password nebula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector + Graph store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleVectorStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual data into all_nodes_embeddded\n",
    "with open(os.path.expanduser('~/scratch-llm/storage/nodes/all_nodes_all-mpnet-base-v2.pkl'), 'rb') as f:\n",
    "    all_nodes_embedded: List[TextNode] = pickle.load(f)\n",
    "# Create dictionaries from the nodes\n",
    "embedding_dict = {node.id_: node.get_embedding() for node in all_nodes_embedded}\n",
    "text_id_to_ref_doc_id = {node.id_: node.ref_doc_id or \"None\" for node in all_nodes_embedded}\n",
    "metadata_dict = {node.id_: node.metadata for node in all_nodes_embedded}\n",
    "\n",
    "# Initialize the SimpleVectorStore with the dictionaries\n",
    "vector_store = SimpleVectorStore(\n",
    "    data = SimpleVectorStoreData(\n",
    "        embedding_dict=embedding_dict,\n",
    "        text_id_to_ref_doc_id=text_id_to_ref_doc_id,\n",
    "        metadata_dict=metadata_dict,\n",
    "    ),\n",
    "    stores_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NebulaPropertyGraphStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = NebulaPropertyGraphStore(\n",
    "    space = \"PrimeKG\",\n",
    "    username = \"root\",\n",
    "    password = \"nebula\",\n",
    "    url = \"nebula://localhost:9669\",\n",
    "    props_schema= \"\"\"`node_index` STRING, `node_type` STRING, `node_id` STRING, `node_name` STRING, \n",
    "        `node_source` STRING, `mondo_id` STRING, `mondo_name` STRING, `group_id_bert` STRING, \n",
    "        `group_name_bert` STRING, `orphanet_prevalence` STRING, `display_relation` STRING \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\", padding_side=\"left\", device_map=\"auto\")    \n",
    "# if tokenizer.pad_token_id is None: #no <pad> token previously defined, only eos_token\n",
    "#     tokenizer.pad_token = \"<|end_of_text|>\"\n",
    "#     tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# if tokenizer.eos_token_id is None: # no <eos> token previously defined\n",
    "#     tokenizer.eos_token = \"<|end_of_text|>\"\n",
    "#     tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name=\"allenai/scibert_scivocab_cased\",\n",
    "#     context_window=512,\n",
    "#     max_new_tokens=1048,\n",
    "#     generate_kwargs={\n",
    "#         \"temperature\": 0.10, \n",
    "#         \"do_sample\": True,\n",
    "#         \"pad_token_id\": tokenizer.pad_token_id,\n",
    "#         \"top_k\": 5, \n",
    "#         \"top_p\": 0.85\n",
    "#     },\n",
    "#     model_kwargs={\n",
    "#         \"torch_dtype\": torch.float16,\n",
    "#     },\n",
    "#     tokenizer=tokenizer,\n",
    "#     # device_map=\"auto\",  # Automatically offload layers to CPU if GPU memory is insufficient\n",
    "#     device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     stopping_ids=[tokenizer.eos_token_id],\n",
    "#     tokenizer_kwargs={\"max_length\": None},\n",
    "#     is_chat_model=True,\n",
    "# )\n",
    "\n",
    "# Settings.llm = llm\n",
    "# Settings.chunk_size = 1024\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # BAAI/bge-small-en-v1.5 /  m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2646e371b4dcf91c586c0fc258698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ebebec687545e4959476b2c92a6e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289b01ae3f66448a96712ec1276eef16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d369fb3bd140d4bc91a74a606f10c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cebee3e4cb48f292533b63b5058ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29b4939dc0a4d63b66f1d8387e83575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2525a31a8b420387f7daac7fca2667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc74e1e5b81403e8c5f9445a7114901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf87fb2871604cbc9b6fd57d30496fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fefbd8820044b8877424c4fc9c4605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1f6b74a9a04e6fb977a347153a566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da054ba19dd84d94bfa1901b31930cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", padding_side=\"left\", device_map=\"auto\")    \n",
    "if tokenizer.pad_token_id is None: #no <pad> token previously defined, only eos_token\n",
    "    tokenizer.pad_token = \"<|end_of_text|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    context_window=8192,\n",
    "    max_new_tokens=3048,\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.10, \n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"top_k\": 10, \n",
    "        \"top_p\": 0.9,\n",
    "        # \"repetition_penalty\": 0.9,  # Added to reduce repetition\n",
    "        # \"no_repeat_ngram_size\": 3,  # Prevents repetition of n-grams\n",
    "    },\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    "    # device_map=\"auto\",  # Automatically offload layers to CPU if GPU memory is insufficient\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    stopping_ids=[tokenizer.eos_token_id],\n",
    "    tokenizer_kwargs={\"max_length\": None},\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # BAAI/bge-small-en-v1.5 /  m3 / sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-3.3-70B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for Llama-3.3-70B-Instruct\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\", padding_side=\"left\", device_map=\"auto\")    \n",
    "# if tokenizer.pad_token_id is None: #no <pad> token previously defined, only eos_token\n",
    "#     tokenizer.pad_token = \"<|end_of_text|>\"\n",
    "#     tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "#     context_window=1048,\n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\n",
    "#         \"temperature\": 0.10, \n",
    "#         \"do_sample\": True,\n",
    "#         \"pad_token_id\": tokenizer.pad_token_id,\n",
    "#         \"top_k\": 5, \n",
    "#         \"top_p\": 0.85\n",
    "#     },\n",
    "#     model_kwargs={\n",
    "#         \"torch_dtype\": torch.float16,\n",
    "#         \"quantization_config\": quantization_config,\n",
    "#     },\n",
    "#     tokenizer=tokenizer,\n",
    "#     # device_map=\"auto\",  # Automatically offload layers to CPU if GPU memory is insufficient\n",
    "#     device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     stopping_ids=[tokenizer.eos_token_id],\n",
    "#     tokenizer_kwargs={\"max_length\": None},\n",
    "#     is_chat_model=True,\n",
    "# )\n",
    "\n",
    "# Settings.llm = llm\n",
    "# Settings.chunk_size = 1024\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # BAAI/bge-small-en-v1.5 /  m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiseaseMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MetadataFilters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataFilters, FilterOperator\n",
    "disease_dict = {\n",
    "    \"key\": \"node_type\",\n",
    "    \"value\": \"disease\",\n",
    "    \"operator\": FilterOperator.EQ\n",
    "}\n",
    "disease_filter = MetadataFilters(filters=[disease_dict])\n",
    "\n",
    "class DiseaseMode():\n",
    "    def __init__(self, vector_store: SimpleVectorStore, graph_store: NebulaPropertyGraphStore):\n",
    "        self.vector_store = vector_store\n",
    "        self.graph_store = graph_store\n",
    "\n",
    "    def retrieve(self, query: str):        \n",
    "        query_embedding = Settings.embed_model.get_text_embedding(query)\n",
    "        vector_results = self.vector_store.query(\n",
    "            VectorStoreQuery(\n",
    "                query_embedding=query_embedding, \n",
    "                similarity_top_k=1,\n",
    "                filters=disease_filter,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        top_node_id = vector_results.ids[0]\n",
    "        top_node_score = vector_results.similarities[0]\n",
    "        kg_node = self.graph_store.get(ids=[top_node_id])[0]\n",
    "                \n",
    "        results = [{ # Create results list with primary node\n",
    "            \"node_index\": kg_node.id_,\n",
    "            \"node_name\": kg_node.properties[\"node_name\"],\n",
    "            \"text\": getattr(kg_node, \"text\", \"\"),\n",
    "            \"score\": top_node_score\n",
    "        }]\n",
    "        \n",
    "        print(f\"Best node from vector query: Node ID: {kg_node.id_}, \"\n",
    "              f\"Score: {top_node_score:.4f}, Name: {kg_node.properties['node_name']}\")\n",
    "        \n",
    "        # Find related nodes through graph query\n",
    "        graph_nodes = self.graph_store.structured_query(\n",
    "            \"\"\"\n",
    "            MATCH (e:Node__) WHERE id(e) == $ids\n",
    "            MATCH p=(e)-[r:Relation__{label:\"disease-disease\"}]-(t) \n",
    "            UNWIND relationships(p) as rel\n",
    "            RETURN DISTINCT id(t), t.Props__.node_name, t.Chunk__.text\n",
    "            \"\"\", \n",
    "            param_map={\"ids\": top_node_id}\n",
    "        )\n",
    "        \n",
    "        # Calculate similarity for related nodes and add relevant ones to results\n",
    "        all_similarities = []\n",
    "        for node in graph_nodes:\n",
    "            node_text = node[\"t.Props__.node_name\"] + \": \" + node[\"t.Chunk__.text\"]\n",
    "            node_embedding = Settings.embed_model.get_text_embedding(node_text)\n",
    "                    \n",
    "            similarity = dot(query_embedding, node_embedding) / (norm(query_embedding) * norm(node_embedding))\n",
    "            all_similarities.append((node, similarity))\n",
    "            \n",
    "        if len(all_similarities) > 3:\n",
    "            sim = [s for _, s in all_similarities]\n",
    "            threshold = np.percentile(sim, 75) # keep top 25% of nodes\n",
    "        else:\n",
    "            threshold = 0.7\n",
    "        \n",
    "        for node, similarity in all_similarities:\n",
    "            if similarity > threshold: #and similarity >= 0.5:\n",
    "                results.append({\n",
    "                    \"node_index\": node[\"id(t)\"],\n",
    "                    \"node_name\": node[\"t.Props__.node_name\"],\n",
    "                    \"text\": node_text,\n",
    "                    \"score\": similarity\n",
    "                })\n",
    "        \n",
    "        results = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "        print(\"\\nBest related nodes from graph query:\")\n",
    "        for node in results:  # Skip primary node\n",
    "            print(f\"ID: {node['node_index']} | node name: {node['node_name']} | score: {node['score']:.4f}\")\n",
    "        \n",
    "        graph_phenotype = graph_store.structured_query(\n",
    "            \"\"\"\n",
    "            MATCH (e:Node__) WHERE id(e) == $ids\n",
    "            MATCH (e)-[r:Relation__{label:\"disease-phenotype-positive\"}]-(t) \n",
    "            RETURN DISTINCT id(t), t.Props__.node_name\n",
    "            \"\"\", \n",
    "            param_map={\"ids\": top_node_id}\n",
    "        )\n",
    "        # join the phenotype names without the \" '' \" characters\n",
    "        phenotypes = \", \".join(node[\"t.Props__.node_name\"].replace(\"'\", \"\") for node in graph_phenotype)\n",
    "\n",
    "        nodes_with_text = [node for node in results if node['text'].strip()]\n",
    "        context = [f\"'{node['node_name']}': {node['text']}\" for node in nodes_with_text] if nodes_with_text else None\n",
    "        phenotype_context = [f\"Is associated with the following phenotypes: {phenotypes}\\n\"] if phenotypes else None\n",
    "        \n",
    "        if results:\n",
    "            return (context, phenotype_context, top_node_id)\n",
    "        else:\n",
    "            return [f\"No graph relationships found for {results[0]['node_name']}\"] if results else [\"No results found\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_phenotype_template = \"\"\"    \n",
    "    Context information is below:\n",
    "    {text_chunks}\n",
    "\n",
    "    Phenotype context is below:\n",
    "    {phenotype_context}\n",
    "\n",
    "    You are a medical knowledge assistant specializing in rare diseases. Your task is to create a comprehensive list of symptoms for {query_str}.\n",
    "\n",
    "    CRITICAL INSTRUCTIONS:\n",
    "    1. Use the information from the context and your own knowledge to provide a comprehensive answer\n",
    "    2. Return MAXIMUM the 16 most relevant symptoms, if there are more than 16 symptoms, return the most relevant ones\n",
    "    3. Use HPO medical terminology and avoid using including redundant symptoms\n",
    "    4. Return EXACTLY this JSON format (no variations):\n",
    "\n",
    "    Always format your response as a VALID JSON:\n",
    "    {\n",
    "        \"disease\": \"{query_str}\",\n",
    "        \"symptoms\": [\n",
    "            \"name symptom1 using HPO terminology\",\n",
    "            \"name symptom2 using HPO terminology\",\n",
    "            ... and so on\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Do NOT use nested objects. Use exactly \"disease\" and \"symptoms\" as shown.\n",
    "\"\"\"\n",
    "\n",
    "no_rag_template = \"\"\"\n",
    "    You are a medical knowledge assistant specializing in rare diseases. Your task is to create a comprehensive list of symptoms for {query_str}.\n",
    "\n",
    "    CRITICAL INSTRUCTIONS:\n",
    "    1. Use only your own knowledge to provide a comprehensive answer\n",
    "    2. Return MAXIMUM the 16 most relevant symptoms, if there are more than 16 symptoms, return the most relevant ones\n",
    "    3. Use only HPO medical terminology and avoid including redundant symptoms\n",
    "    4. Return EXACTLY this JSON format (no variations):\n",
    "\n",
    "    Always format your response as a VALID JSON:\n",
    "    {\n",
    "        \"disease\": \"{query_str}\",\n",
    "        \"symptoms\": [\n",
    "            \"name symptom1 using HPO terminology\",\n",
    "            \"name symptom2 using HPO terminology\",\n",
    "            \"name symptom3 using HPO terminology\",\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Do NOT use nested objects. Use exactly \"disease\" and \"symptoms\" as shown.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG === \n",
      "User query: Developmental and epileptic encephalopathy 4\n",
      "\n",
      "Best node from vector query: Node ID: 28987, Score: 0.7868, Name: developmental and epileptic encephalopathy, 85, with or without midline brain defects\n",
      "\n",
      "Best related nodes from graph query:\n",
      "ID: 28987 | node name: developmental and epileptic encephalopathy, 85, with or without midline brain defects | score: 0.7868\n",
      "ID: 28005 | node name: developmental and epileptic encephalopathy | score: 0.7537\n",
      "Context: [\"'developmental and epileptic encephalopathy': developmental and epileptic encephalopathy: Any earl\n",
      "Phenotypes: ['Is associated with the following phenotypes: Cleft palate, Retrognathia, Posteriorly rotated ears, Anteverted nares, Autistic behavior, Growth delay, Hypertonia, Delayed speech and language development, Stereotypy, Atrial septal defect, Talipes, Camptodactyly, Clinodactyly, Seizure, Hirsutism, Focal-onset seizure, Generalized hypotonia, Overfolded helix, Facial asymmetry, Choanal atresia, Widely spaced teeth, Hypoplasia of the corpus callosum, Bilateral tonic-clonic seizure, Broad hallux, Intrauterine growth retardation, Muscular hypotonia of the trunk, Congenital diaphragmatic hernia, EEG abnormality, Global developmental delay, Partial anomalous pulmonary venous return, Thin upper lip vermilion, Highly arched eyebrow, Hypsarrhythmia, Gastroesophageal reflux, Severe global developmental delay, Low anterior hairline, Round face, Smooth philtrum, Short philtrum, Triangular face, Narrow forehead, Narrow nose, Downslanted palpebral fissures, Upslanted palpebral fissure, Hypotelorism, Synophrys, Dental crowding, Poor eye contact, Tapered finger, Spastic tetraparesis, Absent speech, Hip dysplasia, X-linked dominant inheritance, Short foot, Semilobar holoprosencephaly, Downturned corners of mouth, Short nose, Congenital onset, Single median maxillary incisor, Contracture of the proximal interphalangeal joint of the 3rd finger, 1-2 toe syndactyly, Midface retrusion, Infantile spasms, Multifocal seizures, Delayed ability to walk, Seizure cluster, Small hand, Unilateral ptosis\\n']\n",
      "\n",
      "19 text chunks after repacking\n",
      "1 text chunks after repacking\n",
      "Top node ID: 28987\n",
      "\n",
      "RESPONSE OK: {\"disease\":\"Developmental and epileptic encephalopathy 4\",\"symptoms\":[\"Severe global developmental delay\",\"Global developmental delay\",\"Hypotonia\",\"Muscular hypotonia of the trunk\",\"Hypotonia\",\"Spastic tetraparesis\",\"Seizure\",\"Focal-onset seizure\",\"Bilateral tonic-clonic seizure\",\"Multifocal seizures\",\"Infantile spasms\",\"EEG abnormality\",\"Hypsarrhythmia\",\"Hypotelorism\",\"Synophrys\",\"Poor eye contact\",\"Absent speech\",\"Delayed speech and language development\"]}\n",
      "Symptoms retrieved: 18 for Developmental and epileptic encephalopathy 4\n",
      "\n",
      "\n",
      " === no RAG === \n",
      "User query: Developmental and epileptic encephalopathy 4\n",
      "\n",
      "\n",
      "RESPONSE OK: {\"disease\":\"Developmental and epileptic encephalopathy 4\",\"symptoms\":[\"encephalopathy\",\"epilepsy\",\"developmental delay\",\"seizures\",\"hypotonia\",\"hypomyelination\",\"microcephaly\",\"cerebral atrophy\",\"cerebral edema\",\"neuropathy\",\"autism\",\" intellectual disability\",\"speech impairment\",\"apnea\",\"hypersomnolence\",\"feeding difficulties\",\"failure to thrive\"]}\n",
      "Symptoms retrieved: 17 for Developmental and epileptic encephalopathy 4\n"
     ]
    }
   ],
   "source": [
    "user = \"Developmental and epileptic encephalopathy 4\" # \"autosomal recessive polycystic kidney disease\"\n",
    "print(f\"\\n=== RAG === \\nUser query: {user}\\n\")\n",
    "context, phenotypes, top_node_id = DiseaseMode(vector_store, graph_store).retrieve(user)\n",
    "\n",
    "class Output(BaseModel):\n",
    "    disease: str = Field(description=\"The disease of interest given by the user.\")\n",
    "    symptoms: List[str] = Field(description=\"Symptoms associated with the disease.\")\n",
    "output_parser = PydanticOutputParser(Output)\n",
    "\n",
    "def get_prompt_and_inputs(context, phenotypes):\n",
    "    prompt_template = PromptTemplate(context_phenotype_template)\n",
    "    if context and phenotypes:\n",
    "        return prompt_template, \"\\n\".join(context), phenotypes\n",
    "    elif context and not phenotypes:\n",
    "        return prompt_template, \"\\n\".join(context), \"\"\n",
    "    elif not context and phenotypes:\n",
    "        return prompt_template, \"\", phenotypes\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "prompt_template, text_chunks, phenotype_context = get_prompt_and_inputs(context, phenotypes)\n",
    "\n",
    "if prompt_template:\n",
    "    print(f\"Context: {str(context)[:100]}\" if context is not None else context)\n",
    "    print(f\"Phenotypes: {phenotypes[:100] if phenotypes is not None else phenotypes}\\n\")\n",
    "    summarizer = TreeSummarize(verbose=True, llm=llm, summary_template=prompt_template)\n",
    "    response = summarizer.get_response(\n",
    "        query_str=user,\n",
    "        text_chunks=text_chunks,\n",
    "        phenotype_context=phenotype_context\n",
    "    )\n",
    "else:\n",
    "    print(f\"No context or phenotypes found for {user}\")\n",
    "    response = \"No context or phenotypes found for the given disease.\"\n",
    "\n",
    "try:\n",
    "    rag_response = output_parser.parse(response)\n",
    "    print(\"\\nRESPONSE OK:\", rag_response.model_dump_json())\n",
    "    print(f\"Symptoms retrieved: {len(rag_response.symptoms)} for {rag_response.disease}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning ERROR: {e}\")\n",
    "    # Fallback: create an Output object with empty symptoms and the disease name\n",
    "    rag_response = Output(disease=user, symptoms=[])\n",
    "    print(\"\\n\", rag_response.model_dump_json())\n",
    "    print(f\"{len(rag_response.symptoms)} symptoms for {rag_response.disease}\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\n === no RAG === \\nUser query: {user}\\n\")\n",
    "# include a prompt template for the LLM chat \n",
    "template = PromptTemplate(no_rag_template)\n",
    "prompt = template.format(query_str=user)\n",
    "response = llm.chat([ChatMessage(role=\"user\", content=prompt)])\n",
    "\n",
    "# Extract the text content from the ChatResponse object\n",
    "response_text = response.message.content if hasattr(response, \"message\") else str(response)\n",
    "\n",
    "try:\n",
    "    no_rag_response = output_parser.parse(response_text)\n",
    "    print(\"\\nRESPONSE OK:\", no_rag_response.model_dump_json())\n",
    "    print(f\"Symptoms retrieved: {len(no_rag_response.symptoms)} for {no_rag_response.disease}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning ERROR: {e}\")\n",
    "    # Fallback: create an Output object with empty symptoms and the disease name\n",
    "    no_rag_response = Output(disease=user, symptoms=[])\n",
    "    print(\"\\n\", no_rag_response.model_dump_json())\n",
    "    print(f\"{len(no_rag_response.symptoms)} symptoms for {no_rag_response.disease}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the disease_to_symptoms dictionary from a json file\n",
    "output_file = os.path.expanduser('~/scratch-llm/storage/phenopackets/phenopacket_data.json')\n",
    "with open(output_file, 'r') as f:\n",
    "    phenopackets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common symptoms: set()\n",
      "Common symptoms between RAG and phenopackets: {'delayed speech and language development', 'eeg abnormality', 'seizure', 'hypotonia', 'bilateral tonic-clonic seizure', 'severe global developmental delay', 'global developmental delay', 'multifocal seizures', 'infantile spasms', 'hypsarrhythmia', 'absent speech', 'focal-onset seizure'}\n",
      "Common symptoms between no-RAG and phenopackets: {'hypotonia', 'microcephaly', 'apnea', 'cerebral atrophy', 'feeding difficulties', 'autism', 'encephalopathy'}\n"
     ]
    }
   ],
   "source": [
    "# compare the the symptoms from the RAG and no-RAG responses\n",
    "\n",
    "common_symptoms = set(rag_response.symptoms).intersection(set(no_rag_response.symptoms))\n",
    "print(f\"\\nCommon symptoms: {common_symptoms}\")\n",
    "\n",
    "\n",
    "# compare the two responses with the phenopackets dictionary\n",
    "# Find the entry in phenopackets where the disease name matches the user\n",
    "phenopacket_diseases = phenopackets[user]\n",
    "# Convert all symptoms to lowercase for case-insensitive comparison\n",
    "\n",
    "print(f\"Common symptoms between RAG and phenopackets: {set(s.lower() for s in rag_response.symptoms).intersection(set(d.lower() for d in phenopacket_diseases))}\")\n",
    "print(f\"Common symptoms between no-RAG and phenopackets: {set(s.lower() for s in no_rag_response.symptoms).intersection(set(d.lower() for d in phenopacket_diseases))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the terms in rag_response.symptoms and no_rag_response.symptoms that are in the phenopackets dictionary\n",
    "rag_response.symptoms = [s for s in rag_response.symptoms if s.lower() not in {d.lower() for d in phenopacket_diseases}]\n",
    "no_rag_response.symptoms = [s for s in no_rag_response.symptoms if s.lower() not in {d.lower() for d in phenopacket_diseases}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: Hypomanganesemia | Vector ID: 91569 | KG node: Hypomanganesemia | Similarity: 0.8117\n",
      "Term: Encephalomalacia | Vector ID: 92874 | KG node: Encephalomalacia | Similarity: 0.8392\n",
      "Term: Sensory neuropathy | Vector ID: 23456 | KG node: Sensory neuropathy | Similarity: 0.8459\n",
      "Term: Peripheral hypomyelination | Vector ID: 86942 | KG node: Peripheral hypomyelination | Similarity: 0.9417\n",
      "Term: Cerebral edema | Vector ID: 84424 | KG node: Cerebral edema | Similarity: 0.8580\n",
      "Term: Microlissencephaly | Vector ID: 94301 | KG node: Microlissencephaly | Similarity: 0.8602\n",
      "Term: Poor eye contact | Vector ID: 84722 | KG node: Poor eye contact | Similarity: 0.7474\n",
      "Term: Synophrys | Vector ID: 84681 | KG node: Synophrys | Similarity: 0.6477\n",
      "Term: Muscular hypotonia of the trunk | Vector ID: 24642 | KG node: Muscular hypotonia of the trunk | Similarity: 0.9504\n",
      "Term: Hypotelorism | Vector ID: 84663 | KG node: Hypotelorism | Similarity: 0.7622\n",
      "Term: Seizure cluster | Vector ID: 92487 | KG node: Seizure cluster | Similarity: 0.8649\n",
      "\n",
      "Common symptoms (after clean-up): set()\n",
      "\n",
      "Common symptoms between RAG and phenopackets (after clean up): set()\n",
      "Common symptoms between no-RAG and phenopackets: set()\n"
     ]
    }
   ],
   "source": [
    "phenotype_dict = {\n",
    "    \"key\": \"node_type\",\n",
    "    \"value\": \"effect/phenotype\",\n",
    "    \"operator\": FilterOperator.EQ\n",
    "}\n",
    "phenotype_filter = MetadataFilters(filters=[phenotype_dict])\n",
    "\n",
    "\n",
    "def find_HPO_embedding(symptoms: List[str]) -> List[str]:\n",
    "    \"\"\"Get the embedding for each phenotype term that has not been previously matched and map it to the the most similar HPO term.\"\"\"\n",
    "\n",
    "    new_symptoms = []\n",
    "    for term in symptoms.symptoms:\n",
    "        query_embedding = Settings.embed_model.get_text_embedding(term)\n",
    "        vector_results = vector_store.query(\n",
    "            VectorStoreQuery(\n",
    "                    query_embedding=query_embedding, \n",
    "                    similarity_top_k=1,\n",
    "                    filters=phenotype_filter,\n",
    "                )\n",
    "            )\n",
    "        kg_node = graph_store.get(ids=[vector_results.ids[0]])\n",
    "\n",
    "        # indicate if querying rag or no-rag\n",
    "        print(f\"Term: {term} | Vector ID: {vector_results.ids[0]} | KG node: {kg_node[0].properties['node_name']} | Similarity: {vector_results.similarities[0]:.4f}\")\n",
    "        \n",
    "        # replace the term in no_rag_response.symptoms with the KG node nam\n",
    "        if vector_results.similarities[0] > 0.5 and kg_node:\n",
    "            new_symptoms.append(kg_node[0].properties['node_name'])\n",
    "        else:\n",
    "            new_symptoms.append(term)\n",
    "            \n",
    "    return list(set(new_symptoms))\n",
    "\n",
    "no_rag_response.symptoms = find_HPO_embedding(no_rag_response)\n",
    "rag_response.symptoms = find_HPO_embedding(rag_response)\n",
    "\n",
    "common_symptoms = set(rag_response.symptoms).intersection(set(no_rag_response.symptoms))\n",
    "print(f\"\\nCommon symptoms (after clean-up): {common_symptoms}\")\n",
    "\n",
    "# compare the two responses again with the phenopackets dictionary\n",
    "print(f\"\\nCommon symptoms between RAG and phenopackets (after clean up): {set(s.lower() for s in rag_response.symptoms).intersection(set(d.lower() for d in phenopacket_diseases))}\")\n",
    "print(f\"Common symptoms between no-RAG and phenopackets: {set(s.lower() for s in no_rag_response.symptoms).intersection(set(d.lower() for d in phenopacket_diseases))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO match for HP:0032096 - Abnormal manganese concentration\n",
      "Match found HP:0009830 - Peripheral neuropathy\n",
      "NO match for HP:0003130 - Abnormal peripheral myelination\n",
      "NO match for HP:0000969 - Edema\n",
      "NO match for HP:0002060 - Abnormal cerebral morphology\n",
      "NO match for HP:0001339 - Lissencephaly\n",
      "NO match for HP:0002060 - Abnormal cerebral morphology\n"
     ]
    }
   ],
   "source": [
    "from pyhpo import Ontology\n",
    "_ = Ontology()\n",
    "# Compare the parent terms of rag_response.symptoms to the ones in phenopacket_diseases\n",
    "for term in rag_response.symptoms:\n",
    "    hpo_term = Ontology.get_hpo_object(term)\n",
    "    if hpo_term:\n",
    "        if hpo_term.parents:\n",
    "            for parent in hpo_term.parents:\n",
    "                parent_name = parent.name.lower()\n",
    "                if parent_name in {d.lower() for d in phenopacket_diseases}:\n",
    "                    print(f\"Match found {parent.id} - {parent.name}\")\n",
    "                else:\n",
    "                    print(f\"NO match for {parent.id} - {parent.name}\")\n",
    "        else:\n",
    "            print(f\"Term: {hpo_term.name} | Parent: None\")\n",
    "    else:\n",
    "        print(f\"Term {term} not found in HPO ontology.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks with NebulaGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS PrimeKG(vid_type=FIXED_STRING(256))\n",
    "%ngql USE PrimeKG;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e.Props__.node_name</th>\n",
       "      <th>e.Chunk__.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wieacker-Wolff syndrome (spectrum)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  e.Props__.node_name e.Chunk__.text\n",
       "0  Wieacker-Wolff syndrome (spectrum)               "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "MATCH (e:Node__)\n",
    "WHERE id(e) == \"38849\"\n",
    "RETURN DISTINCT \n",
    "        e.Props__.node_name, \n",
    "        e.Chunk__.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e.Props__.node_name</th>\n",
       "      <th>e.Props__.node_type</th>\n",
       "      <th>e.Chunk__.text</th>\n",
       "      <th>t1.Props__.node_name</th>\n",
       "      <th>t1.Props__.node_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>familial focal alopecia</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Autosomal dominant inheritance</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>familial focal alopecia</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Patchy alopecia</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>myelolymphatic insufficiency</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Hyposegmentation of neutrophil nuclei</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>myelolymphatic insufficiency</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>X-linked recessive inheritance</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cerebellar, ocular, craniofacial, and genital syndrome</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Low-set ears</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>Wieacker-Wolff syndrome, female-restricted</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Inability to walk</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>Wieacker-Wolff syndrome, female-restricted</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Distal muscle weakness</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>Wieacker-Wolff syndrome, female-restricted</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Kyphosis</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>Wieacker-Wolff syndrome, female-restricted</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Flexion contracture</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>Wieacker-Wolff syndrome, female-restricted</td>\n",
       "      <td>disease</td>\n",
       "      <td></td>\n",
       "      <td>Generalized hypotonia</td>\n",
       "      <td>effect/phenotype</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12454 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          e.Props__.node_name e.Props__.node_type e.Chunk__.text                   t1.Props__.node_name t1.Props__.node_type\n",
       "0                                     familial focal alopecia             disease                        Autosomal dominant inheritance     effect/phenotype\n",
       "1                                     familial focal alopecia             disease                                       Patchy alopecia     effect/phenotype\n",
       "2                                myelolymphatic insufficiency             disease                 Hyposegmentation of neutrophil nuclei     effect/phenotype\n",
       "3                                myelolymphatic insufficiency             disease                        X-linked recessive inheritance     effect/phenotype\n",
       "4      cerebellar, ocular, craniofacial, and genital syndrome             disease                                          Low-set ears     effect/phenotype\n",
       "...                                                       ...                 ...            ...                                    ...                  ...\n",
       "12449              Wieacker-Wolff syndrome, female-restricted             disease                                     Inability to walk     effect/phenotype\n",
       "12450              Wieacker-Wolff syndrome, female-restricted             disease                                Distal muscle weakness     effect/phenotype\n",
       "12451              Wieacker-Wolff syndrome, female-restricted             disease                                              Kyphosis     effect/phenotype\n",
       "12452              Wieacker-Wolff syndrome, female-restricted             disease                                   Flexion contracture     effect/phenotype\n",
       "12453              Wieacker-Wolff syndrome, female-restricted             disease                                 Generalized hypotonia     effect/phenotype\n",
       "\n",
       "[12454 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "# find all disease nodes where e.Chunk__.text is empty\n",
    "MATCH (e:Node__)\n",
    "MATCH (e)-[r1:Relation__{label:\"disease-phenotype-positive\"}]-(t1) # {label:\"disease-phenotype-positive\"}\n",
    "WHERE e.Props__.node_type == \"disease\" AND e.Chunk__.text == \"\"\n",
    "RETURN DISTINCT \n",
    "    e.Props__.node_name,\n",
    "    e.Props__.node_type,\n",
    "    e.Chunk__.text,\n",
    "    t1.Props__.node_name,\n",
    "    t1.Props__.node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%ngql\n",
    "# MATCH (e:Node__)\n",
    "# WHERE id(e) == \"27781\"\n",
    "# MATCH (e)-[r1:Relation__{label:\"disease-phenotype-positive\"}]-(t1) # {label:\"disease-phenotype-positive\"}\n",
    "# # MATCH (e)-[r2:Relation__{label:\"disease-disease\"}]-(t2)\n",
    "# # RETURN e.Props__.node_name, e.Chunk__.text\n",
    "\n",
    "# RETURN DISTINCT \n",
    "#         e.Props__.node_name, \n",
    "#         # e.Chunk__.text\n",
    "#         r1.label,\n",
    "#         id(t1),\n",
    "#         t1.Props__.node_name\n",
    "#         # r2.label,\n",
    "#         # id(t2) AS index_disease,\n",
    "#         # t2.Props__.node_name AS name_disease\n",
    "#         # t.Chunk__.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;47;75;124mudocker command: udocker /home/lasa14/.conda/envs/llm-rag/bin/udocker --allow-root ps | grep graphd | awk '{print $1}' | xargs -I {} udocker --allow-root rm -f {}\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145merror: Error: deleting directory:  /home/lasa14/.udocker/containers/d9a1b12a-62b5-3764-80e3-dd482130cc1d\n",
      "Error: deleting container\n",
      "\u001b[0m\n",
      "Retrying in 5 seconds... udocker command failed with return code 123\n",
      "\u001b[1;3;38;2;47;75;124mudocker command: udocker /home/lasa14/.conda/envs/llm-rag/bin/udocker --allow-root ps | grep graphd | awk '{print $1}' | xargs -I {} udocker --allow-root rm -f {}\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145merror: Error: deleting directory:  /home/lasa14/.udocker/containers/d9a1b12a-62b5-3764-80e3-dd482130cc1d\n",
      "Error: deleting container\n",
      "\u001b[0m\n",
      "Retrying in 15 seconds... udocker command failed with return code 123\n",
      "\u001b[1;3;38;2;47;75;124mudocker command: udocker /home/lasa14/.conda/envs/llm-rag/bin/udocker --allow-root ps | grep graphd | awk '{print $1}' | xargs -I {} udocker --allow-root rm -f {}\u001b[0m\n",
      "\u001b[1;3;38;2;102;81;145merror: Error: deleting directory:  /home/lasa14/.udocker/containers/d9a1b12a-62b5-3764-80e3-dd482130cc1d\n",
      "Error: deleting container\n",
      "\u001b[0m\n",
      "Process with PID 535765 has been terminated.\n",
      "Process with PID 534711 has been terminated.\n"
     ]
    }
   ],
   "source": [
    "n.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
