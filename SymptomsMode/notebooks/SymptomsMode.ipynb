{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72960817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os, torch, pickle, time, json\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['HF_HOME'] = str(pathlib.Path(\"~/scratch-llm/storage/cache/huggingface/\").expanduser().absolute()) # '/scratch-llm/storage/cache/'\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"~/scratch-llm/storage/models/\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from nebulagraph_lite import nebulagraph_let as ng_let\n",
    "from llama_index.graph_stores.nebula import NebulaPropertyGraphStore\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.vector_stores.simple import SimpleVectorStoreData, SimpleVectorStore, VectorStoreQuery\n",
    "from llama_index.core.vector_stores.types import MetadataFilters, FilterOperator\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from typing import List\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c152bfb",
   "metadata": {},
   "source": [
    "# NebulaGraph conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load NebulaGraph JupyterNotebook extension\n",
    "# !udocker pull vesoft/nebula-metad:v3\n",
    "# !udocker create --name=nebula-metad vesoft/nebula-metad:v3\n",
    "# !udocker setup --execmode=F1 nebula-metad\n",
    "# time.sleep(5)  # wait for the container to be ready\n",
    "# !udocker pull vesoft/nebula-storaged:v3\n",
    "# !udocker create --name=nebula-storaged vesoft/nebula-storaged:v3\n",
    "# !udocker setup --execmode=F1 nebula-storaged\n",
    "# time.sleep(5)  # wait for the container to be ready\n",
    "# !udocker pull vesoft/nebula-graphd:v3\n",
    "# !udocker create --name=nebula-graphd vesoft/nebula-graphd:v3\n",
    "# !udocker setup --execmode=F1 nebula-graphd\n",
    "\n",
    "\n",
    "n = ng_let(in_container=True, debug=True)  # Enable debug for more info\n",
    "n.start() # Takes around 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b6fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;0;135;107m[OK] Connection Pool Created\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PrimeKG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basketballplayer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name\n",
       "0           PrimeKG\n",
       "1  basketballplayer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext ngql\n",
    "%ngql --address 127.0.0.1 --port 9669 --user root --password nebula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb28b1",
   "metadata": {},
   "source": [
    "# Vector + Graph store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb5e0c",
   "metadata": {},
   "source": [
    "## SimpleVectorStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8832cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual data into all_nodes_embeddded\n",
    "with open(os.path.expanduser('~/scratch-llm/storage/nodes/all_nodes_all-mpnet-base-v2.pkl'), 'rb') as f:\n",
    "    all_nodes_embedded: List[TextNode] = pickle.load(f)\n",
    "# Create dictionaries from the nodes\n",
    "embedding_dict = {node.id_: node.get_embedding() for node in all_nodes_embedded}\n",
    "text_id_to_ref_doc_id = {node.id_: node.ref_doc_id or \"None\" for node in all_nodes_embedded}\n",
    "metadata_dict = {node.id_: node.metadata for node in all_nodes_embedded}\n",
    "\n",
    "# Initialize the SimpleVectorStore with the dictionaries\n",
    "vector_store = SimpleVectorStore(\n",
    "    data = SimpleVectorStoreData(\n",
    "        embedding_dict=embedding_dict,\n",
    "        text_id_to_ref_doc_id=text_id_to_ref_doc_id,\n",
    "        metadata_dict=metadata_dict,\n",
    "    ),\n",
    "    stores_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716316ff",
   "metadata": {},
   "source": [
    "## NebulaPropertyGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = NebulaPropertyGraphStore(\n",
    "    space = \"PrimeKG\",\n",
    "    username = \"root\",\n",
    "    password = \"nebula\",\n",
    "    url = \"nebula://localhost:9669\",\n",
    "    props_schema= \"\"\"`node_index` STRING, `node_type` STRING, `node_id` STRING, `node_name` STRING, \n",
    "        `node_source` STRING, `mondo_id` STRING, `mondo_name` STRING, `group_id_bert` STRING, \n",
    "        `group_name_bert` STRING, `orphanet_prevalence` STRING, `display_relation` STRING \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15941133",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc99d8",
   "metadata": {},
   "source": [
    "## Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ad96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", padding_side=\"left\", device_map=\"auto\")    \n",
    "if tokenizer.pad_token_id is None: #no <pad> token previously defined, only eos_token\n",
    "    tokenizer.pad_token = \"<|end_of_text|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    context_window=8192,\n",
    "    max_new_tokens=3048,\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.10, \n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"top_k\": 10, \n",
    "        \"top_p\": 0.9,\n",
    "        # \"repetition_penalty\": 0.9,  # Added to reduce repetition\n",
    "        # \"no_repeat_ngram_size\": 3,  # Prevents repetition of n-grams\n",
    "    },\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    "    # device_map=\"auto\",  # Automatically offload layers to CPU if GPU memory is insufficient\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    stopping_ids=[tokenizer.eos_token_id],\n",
    "    tokenizer_kwargs={\"max_length\": None},\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\") # BAAI/bge-small-en-v1.5 /  m3 / sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f00da0",
   "metadata": {},
   "source": [
    "# SymptomsMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_dict = {\n",
    "    \"key\": \"node_type\",\n",
    "    \"value\": \"effect/phenotype\",\n",
    "    \"operator\": FilterOperator.EQ\n",
    "}\n",
    "\n",
    "phenotype_filter = MetadataFilters(filters=[phenotype_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df786c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymptomsMode():\n",
    "    def __init__(self, vector_store: SimpleVectorStore, graph_store: NebulaPropertyGraphStore):\n",
    "        self.vector_store = vector_store\n",
    "        self.graph_store = graph_store\n",
    "\n",
    "    def retrieve(self, query: List[str]):\n",
    "        if not isinstance(query, list):\n",
    "            return None\n",
    "            \n",
    "        from collections import defaultdict\n",
    "        \n",
    "        disease_counter = {}\n",
    "        total_symptoms = len(query)\n",
    "        # print(f\"Processing {total_symptoms} symptoms: {query}\")\n",
    "        \n",
    "        # Collect all diseases and count symptom matches\n",
    "        for symptom in query:\n",
    "            query_embedding = Settings.embed_model.get_text_embedding(symptom)\n",
    "            vector_store_query = VectorStoreQuery(\n",
    "                query_embedding=query_embedding,\n",
    "                similarity_top_k=1,\n",
    "                filters=phenotype_filter,\n",
    "            )\n",
    "            individual_results = vector_store.query(vector_store_query)\n",
    "            \n",
    "            for node_id, score in zip(individual_results.ids, individual_results.similarities):\n",
    "                # Get related diseases from graph\n",
    "                graph_nodes = graph_store.structured_query(\n",
    "                    \"\"\"\n",
    "                    MATCH (e:Node__) WHERE id(e) == $ids\n",
    "                    MATCH p=(e)-[r:Relation__{label:\"disease-phenotype-positive\"}]-(t) \n",
    "                    RETURN DISTINCT id(t), t.Props__.node_name, t.Chunk__.text\n",
    "                    \"\"\", \n",
    "                    param_map={\"ids\": node_id}\n",
    "                )\n",
    "                \n",
    "                # Process each related disease\n",
    "                for node in graph_nodes:\n",
    "                    disease_id = node['id(t)']\n",
    "                    disease_name = node['t.Props__.node_name']\n",
    "                    \n",
    "                    if disease_id not in disease_counter:\n",
    "                        disease_counter[disease_id] = {\n",
    "                            'index': disease_id,\n",
    "                            'name': disease_name,\n",
    "                            'count': 1,\n",
    "                            'symptoms': [symptom]\n",
    "                        }\n",
    "                    else:\n",
    "                        disease_counter[disease_id]['count'] += 1\n",
    "                        if symptom not in disease_counter[disease_id]['symptoms']:\n",
    "                            disease_counter[disease_id]['symptoms'].append(symptom)\n",
    "        \n",
    "        if not disease_counter:\n",
    "            print(\"No diseases found matching any symptoms.\")\n",
    "            return {\n",
    "                'top_diseases': {},\n",
    "                'top_match_counts': [],\n",
    "                'grouped_diseases': {},\n",
    "                'total_symptoms': total_symptoms,\n",
    "                'top_diseases_list': []\n",
    "            }\n",
    "        \n",
    "        # Get top 2 match counts\n",
    "        all_match_counts = sorted(set(data['count'] for data in disease_counter.values()), reverse=True)\n",
    "        top_match_counts = all_match_counts[:2]\n",
    "        \n",
    "        # Filter diseases with top 2 match counts\n",
    "        top_diseases = {\n",
    "            disease_id: data for disease_id, data in disease_counter.items()\n",
    "            if data['count'] in top_match_counts\n",
    "        }\n",
    "        \n",
    "        # Group and sort diseases by match count\n",
    "        grouped_diseases = defaultdict(list)\n",
    "        for disease_id, data in top_diseases.items():\n",
    "            grouped_diseases[data['count']].append((disease_id, data))\n",
    "        \n",
    "        for count in grouped_diseases:\n",
    "            grouped_diseases[count].sort(key=lambda x: x[1]['name'])\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n== Diseases with top 2 symptom match counts ==\")\n",
    "        all_top_diseases_list = []\n",
    "        for count in sorted(grouped_diseases.keys(), reverse=True):\n",
    "            print(f\"\\n--- Diseases with {count}/{total_symptoms} symptom matches ---\")\n",
    "            for disease_id, data in grouped_diseases[count]:\n",
    "                print(f\"ID: {data['index']} | Disease: {data['name']} | Matches: {data['count']}/{total_symptoms}\")\n",
    "                all_top_diseases_list.append(data['name'])\n",
    "        \n",
    "        return {\n",
    "            'top_diseases': top_diseases,\n",
    "            'top_match_counts': top_match_counts,\n",
    "            'grouped_diseases': dict(grouped_diseases),\n",
    "            'total_symptoms': total_symptoms,\n",
    "            'top_diseases_list': all_top_diseases_list\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc374f1",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rag_template = \"\"\"\n",
    "You are a medical knowledge assistant specializing in rare diseases. Your task is to provide a differential diagnosis for the following list of symptoms.\n",
    "List of symptoms: {query_str}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Use the information from the context and your own knowledge to provide a comprehensive answer.\n",
    "2. Return maximum the 10 most relevant diseases, ordered by relevance.\n",
    "3. Use medical terminology to refer to the diseases, without abreviations.\n",
    "4. Return EXACTLY this JSON format:\n",
    "\n",
    "Always format your response as a VALID JSON:\n",
    "    {\n",
    "        \"symptoms\": {query_str},\n",
    "        \"differential_diagnosis\": [\n",
    "            \"disease1\",\n",
    "            \"disease2\",\n",
    "            ... and so on\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Do NOT use nested objects. Use exactly \"disease\" and \"symptoms\" as shown.\n",
    "\"\"\"\n",
    "\n",
    "rag_template = \"\"\"\n",
    "You are a medical knowledge assistant specializing in rare diseases. Your task is to provide a differential diagnosis for the following list of symptoms.\n",
    "List of symptoms: {query_str}\n",
    "\n",
    "Use the following candidate diseases to guide your answer: {text_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Use the information from the context and your own knowledge to provide a comprehensive differential diagnosis.\n",
    "2. Return maximum the 10 most relevant diseases, ordered by relevance.\n",
    "3. Use medical terminology to refer to the diseases, without abreviations.\n",
    "4. Return EXACTLY this JSON format:\n",
    "\n",
    "Always format your response as a VALID JSON:\n",
    "    {\n",
    "        \"symptoms\": {query_str},\n",
    "        \"differential_diagnosis\": [\n",
    "            \"disease1\",\n",
    "            \"disease2\",\n",
    "            ... and so on\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Do NOT use nested objects. Use exactly \"disease\" and \"symptoms\" as shown.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9592c",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfb89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 17 symptoms: ['Proximal muscle weakness', 'Delayed ability to walk', 'Poor head control', 'Talipes', 'Muscular dystrophy', 'Loss of ambulation', 'Tube feeding', 'Paroxysmal atrial tachycardia', 'Hypotonia', 'Ventricular tachycardia', 'Delayed ability to roll over', 'Decreased fetal movement', 'Neck muscle weakness', 'Muscle fiber atrophy', 'Axial muscle weakness', 'Respiratory insufficiency due to muscle weakness', 'Distal muscle weakness']\n",
      "\n",
      "=== Diseases with top 2 symptom match counts ===\n",
      "\n",
      "--- Diseases with 9/17 symptom matches ---\n",
      "ID: 27265 | Disease: congenital myasthenic syndrome | Matches: 9/17\n",
      "ID: 27315 | Disease: limb-girdle muscular dystrophy | Matches: 9/17\n",
      "\n",
      "--- Diseases with 7/17 symptom matches ---\n",
      "ID: 30345 | Disease: Bethlem myopathy | Matches: 7/17\n",
      "ID: 31690 | Disease: congenital muscular dystrophy due to LMNA mutation | Matches: 7/17\n",
      "ID: 27294 | Disease: nemaline myopathy | Matches: 7/17\n",
      "1 text chunks after repacking\n",
      "\n",
      "RESPONSE OK: {\"symptoms\":\"Proximal muscle weakness, Delayed ability to walk, Poor head control, Talipes, Muscular dystrophy, Loss of ambulation, Tube feeding, Paroxysmal atrial tachycardia, Hypotonia, Ventricular tachycardia, Delayed ability to roll over, Decreased fetal movement, Neck muscle weakness, Muscle fiber atrophy, Axial muscle weakness, Respiratory insufficiency due to muscle weakness, Distal muscle weakness\",\"differential_diagnosis\":[\"Becker Muscular Dystrophy\",\"Duchenne Muscular Dystrophy\",\"Lambert-Eaton Myasthenic Syndrome\",\"Myotonic Dystrophy\",\"Congenital Myopathies\",\"Spinal Muscular Atrophy\",\"Kugelberg-Welander Disease\",\"Myotonic Dystrophy Type 1\",\"Myotonic Dystrophy Type 2\",\"Emery-Dreifuss Muscular Dystrophy\"]}\n",
      "Differential diagnosis: ['Becker Muscular Dystrophy', 'Duchenne Muscular Dystrophy', 'Lambert-Eaton Myasthenic Syndrome', 'Myotonic Dystrophy', 'Congenital Myopathies', 'Spinal Muscular Atrophy', 'Kugelberg-Welander Disease', 'Myotonic Dystrophy Type 1', 'Myotonic Dystrophy Type 2', 'Emery-Dreifuss Muscular Dystrophy']\n",
      "\n",
      "\n",
      " == no RAG response ==\n",
      "\n",
      "RESPONSE OK: {\"symptoms\":\"Proximal muscle weakness, Delayed ability to walk, Poor head control, Talipes, Muscular dystrophy, Loss of ambulation, Tube feeding, Paroxysmal atrial tachycardia, Hypotonia, Ventricular tachycardia, Delayed ability to roll over, Decreased fetal movement, Neck muscle weakness, Muscle fiber atrophy, Axial muscle weakness, Respiratory insufficiency due to muscle weakness, Distal muscle weakness\",\"differential_diagnosis\":[\"Duchenne Muscular Dystrophy\",\"Becker Muscular Dystrophy\",\"Spinal Muscular Atrophy\",\"Congenital Myasthenic Syndrome\",\"Myotonic Dystrophy\",\"Lambert-Eaton Myasthenic Syndrome\",\"Congenital Myopathies\",\"Facioscapulohumeral Muscular Dystrophy\",\"Kugelberg-Welander Disease\",\"Myotonic Dystrophy Type 1\"]}\n",
      "Differential diagnosis: ['Duchenne Muscular Dystrophy', 'Becker Muscular Dystrophy', 'Spinal Muscular Atrophy', 'Congenital Myasthenic Syndrome', 'Myotonic Dystrophy', 'Lambert-Eaton Myasthenic Syndrome', 'Congenital Myopathies', 'Facioscapulohumeral Muscular Dystrophy', 'Kugelberg-Welander Disease', 'Myotonic Dystrophy Type 1']\n"
     ]
    }
   ],
   "source": [
    "user = [\"Proximal muscle weakness\",\n",
    "        \"Delayed ability to walk\",\n",
    "        \"Poor head control\",\n",
    "        \"Talipes\",\n",
    "        \"Muscular dystrophy\",\n",
    "        \"Loss of ambulation\",\n",
    "        \"Tube feeding\",\n",
    "        \"Paroxysmal atrial tachycardia\",\n",
    "        \"Hypotonia\",\n",
    "        \"Ventricular tachycardia\",\n",
    "        \"Delayed ability to roll over\",\n",
    "        \"Decreased fetal movement\",\n",
    "        \"Neck muscle weakness\",\n",
    "        \"Muscle fiber atrophy\",\n",
    "        \"Axial muscle weakness\",\n",
    "        \"Respiratory insufficiency due to muscle weakness\",\n",
    "        \"Distal muscle weakness\"\n",
    "]\n",
    "context = SymptomsMode(vector_store, graph_store).retrieve(user)\n",
    "prompt_template = PromptTemplate(rag_template)\n",
    "\n",
    "try:\n",
    "    prompt = prompt_template.format(\n",
    "        query_str=\", \".join(user), \n",
    "        text_chunks=\", \".join([chunk for chunk in context['top_diseases_list']]))\n",
    "    \n",
    "    # print(f\"\\nTemplate: {prompt}\")\n",
    "    response = llm.chat([ChatMessage(role=\"user\", content=prompt)])\n",
    "    response_text = response.message.content if hasattr(response, 'message') else str(response)\n",
    "    display(Markdown(response_text))\n",
    "except ValueError as e:\n",
    "    try: \n",
    "        summarizer = TreeSummarize(verbose=True, llm=llm, summary_template=prompt_template)\n",
    "        response = summarizer.get_response(\n",
    "            query_str=\", \".join(user),\n",
    "            text_chunks=\", \".join([chunk for chunk in context['top_diseases_list']])\n",
    "        )\n",
    "        display(Markdown(response))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while summarizing: {e}\")\n",
    "        display(Markdown(response))\n",
    "\n",
    "\n",
    "print(f\"\\n\\n == no RAG response ==\")\n",
    "template= PromptTemplate(no_rag_template)\n",
    "prompt=template.format(query_str=\", \".join(user))\n",
    "response = llm.chat([ChatMessage(role=\"user\", content=prompt)])\n",
    "\n",
    "response_text = response.message.content if hasattr(response, 'message') else str(response)\n",
    "display(Markdown(response_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01c54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the phenopackets data\n",
    "output_file = os.path.expanduser('~/scratch-llm/storage/phenopackets/phenopacket_data.json')\n",
    "with open(output_file, 'r') as f:\n",
    "    phenopackets = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
