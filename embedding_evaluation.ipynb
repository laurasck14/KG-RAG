{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7740852f",
   "metadata": {},
   "source": [
    "# Embedding evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c33405",
   "metadata": {},
   "source": [
    "## MTEB embedding model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44582d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from mteb import MTEB\n",
    "import mteb, pickle\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3107e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_names = {\"BAAI/bge-small-en-v1.5\",\n",
    "              \"all-mpnet-base-v2\",\n",
    "                \"BAAI/bge-m3\",\n",
    "                \"all-MiniLM-L6-v2\",  \n",
    "                \"all-MiniLM-L12-v2\"}\n",
    "                \n",
    "tasks  = mteb.get_tasks(task_types=[\"Retrieval\"], languages=[\"eng\"], domains=[\"Medical\"]) # tasks=[\"MedicalQARetrieval\", \"PublicHealthQA\"]\n",
    "evaluation = MTEB(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b638a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    model = SentenceTransformer(name)\n",
    "    evaluation = MTEB(tasks)\n",
    "    results = evaluation.run(model, output_folder=f\"results/{name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9346b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "# Define the path to the JSON files\n",
    "json_files = glob.glob(\"/home/lasa14/project-llm/git-llm-rag/results/**/*.json\", recursive=True)\n",
    "\n",
    "# Iterate through each file and extract the \"main_score\"\n",
    "for file in json_files:\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            main_score = data.get(\"scores\", {}).get(\"test\", [{}])[0].get(\"main_score\", None)\n",
    "            if main_score is not None:\n",
    "                print(f\"File: {file}\")\n",
    "                print(f\"Main Score: {main_score}\\n\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ee8ab",
   "metadata": {},
   "source": [
    "### Plot MTEB evaluation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4a515",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "# Prepare data for plotting\n",
    "colors = [\"blue\", \"green\"]  # Assign colors for datasets\n",
    "dataset_names = list(set(dataset for scores in model_scores.values() for dataset in scores.keys()))\n",
    "dataset_colors = {dataset: colors[i % len(colors)] for i, dataset in enumerate(dataset_names)}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Track datasets added to the legend\n",
    "added_to_legend = set()\n",
    "\n",
    "for model_name, datasets in model_scores.items():\n",
    "    for dataset_name, scores in datasets.items():\n",
    "        plt.scatter(\n",
    "            [model_name] * len(scores),\n",
    "            scores,\n",
    "            color=dataset_colors[dataset_name],\n",
    "            label=dataset_name if dataset_name not in added_to_legend else None\n",
    "        )\n",
    "        added_to_legend.add(dataset_name)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Main Score\")\n",
    "plt.title(\"Main Scores by Model and Dataset\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Dataset\", loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f86f3",
   "metadata": {},
   "source": [
    "# Generate subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49957787",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#import nodes from pickle from the storage\n",
    "with open(os.path.expanduser('~/scratch-llm/storage/nodes/all_nodes_embedded_mpnet_base_v2.pkl'), 'rb') as f:\n",
    "    all_nodes_embedded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ea34d",
   "metadata": {},
   "source": [
    "## Find nodes that have edges of multiple types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0fd0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Create a dictionary to map node IDs to nodes for faster lookup\n",
    "node_dict = {node.node_id: node for node in all_nodes_embedded}\n",
    "\n",
    "# Create a dictionary to track connections between node types\n",
    "node_type_connections = defaultdict(set)\n",
    "\n",
    "# Iterate through all nodes and their relationships with a progress bar\n",
    "for node in tqdm(all_nodes_embedded, desc=\"Processing nodes\"):\n",
    "    for rel in node.relationships.get(NodeRelationship.CHILD, []):\n",
    "        target_node = node_dict.get(rel.node_id)  # Use dictionary lookup instead of searching the list\n",
    "        if target_node:\n",
    "            target_node_type = target_node.metadata.get('node_type', 'unknown')\n",
    "            node_type_connections[node.node_id].add(target_node_type)\n",
    "\n",
    "# Find all nodes connected to at least 4 different types\n",
    "def find_nodes_with_four_types(node_type_connections):\n",
    "    nodes_with_four_types = []\n",
    "    for node_id, connected_types in node_type_connections.items():\n",
    "        if len(connected_types) >= 10:\n",
    "            nodes_with_four_types.append((node_id, connected_types))\n",
    "    return nodes_with_four_types\n",
    "\n",
    "nodes_with_four_types = find_nodes_with_four_types(node_type_connections)\n",
    "\n",
    "# Store the nodes with at least 4 different types\n",
    "if nodes_with_four_types:\n",
    "    print(f\"Found {len(nodes_with_four_types)} nodes with connections to at least 4 different types.\")\n",
    "    for node_id, connected_types in nodes_with_four_types:\n",
    "        print(f\"Node ID: {node_id}, Connected Types: {connected_types}\")\n",
    "else:\n",
    "    print(\"No nodes found with connections to at least 4 different types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17802a90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#print index of node_id = 27158\n",
    "\n",
    "node_index = next((i for i, node in enumerate(all_nodes_embedded) if node.node_id == \"12120\"), None)\n",
    "node_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12aca2a",
   "metadata": {},
   "source": [
    "## Obtain the nodes from the subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc2de3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#small subgraph with text nodes\n",
    "start_node = all_nodes_embedded[98971] \n",
    "related_nodes_info = start_node.relationships.get(NodeRelationship.CHILD, [])\n",
    "\n",
    "related_nodes = []\n",
    "edges = []\n",
    "for relation in related_nodes_info:\n",
    "    # Find the related node by its node_id\n",
    "    related_node = next((node for node in all_nodes_embedded if node.node_id == relation.node_id), None)\n",
    "    if related_node:\n",
    "        related_nodes.append(related_node)\n",
    "        edges.append((start_node.node_id, related_node.node_id, relation.metadata))\n",
    "\n",
    "\n",
    "subgraph_nodes = [start_node] + related_nodes  # Include the starting node and its neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a12e9c",
   "metadata": {},
   "source": [
    "plot the subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98df272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)  # trial and error, 15 seems to be the cut off\n",
    "# Get the embeddings and labels\n",
    "embeddings = np.array([node.embedding for node in subgraph_nodes])\n",
    "\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "labels = [node.metadata['node_type'] for node in subgraph_nodes]\n",
    "\n",
    "# Convert labels to numerical values for coloring\n",
    "label_to_num = {label: idx for idx, label in enumerate(set(labels))}\n",
    "numeric_labels = [label_to_num[label] for label in labels]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=numeric_labels, cmap='tab10')\n",
    "\n",
    "# Add a legend on the side\n",
    "legend_labels = list(label_to_num.keys())\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels, title=\"Node Types\", loc=\"upper left\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Visualization of Node Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88811960",
   "metadata": {},
   "source": [
    "plot the diseases only from the subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f889d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from adjustText import adjust_text  # label adjustment\n",
    "\n",
    "# Plot the embeddings according to the \"node_type\" = 'disease' from the \"reduced_embeddings\" variable\n",
    "disease_embeddings = []\n",
    "disease_labels = []\n",
    "for i, node in enumerate(subgraph_nodes):\n",
    "    if node.metadata['node_type'] == 'disease':\n",
    "        disease_embeddings.append(reduced_embeddings[i])\n",
    "        disease_labels.append(node.metadata['node_name'])\n",
    "\n",
    "# Plot the embeddings\n",
    "disease_embeddings = np.array(disease_embeddings)\n",
    "plt.figure(figsize=(15, 12))  # Increase the figure size\n",
    "plt.scatter(disease_embeddings[:, 0], disease_embeddings[:, 1])\n",
    "\n",
    "# Add labels and adjust their positions\n",
    "texts = []\n",
    "for i, txt in enumerate(disease_labels):\n",
    "    texts.append(plt.text(disease_embeddings[i, 0], disease_embeddings[i, 1], txt))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))  # Adjust text positions\n",
    "\n",
    "plt.title(\"Disease Node Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1419673",
   "metadata": {},
   "source": [
    "## distance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b098a5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with a biomedical model if needed\n",
    "\n",
    "# Compute embeddings for node names\n",
    "node_embeddings = model.encode(disease_names)\n",
    "\n",
    "similarity = cosine_similarity(node_embeddings, node_embeddings)\n",
    "np.fill_diagonal(similarity, 0)\n",
    "max_sim = np.max(similarity)\n",
    "max_idx = np.unravel_index(np.argmax(similarity), similarity.shape)\n",
    "print(f\"Max cosine similarity:\", max_sim, (disease_names[max_idx[0]], disease_names[max_idx[1]]))\n",
    "\n",
    "# Compute euclidean distances avoiding self-comparisons\n",
    "distance = euclidean_distances(node_embeddings, node_embeddings)\n",
    "np.fill_diagonal(distance, np.inf)\n",
    "min_dist = np.min(distance)\n",
    "min_idx = np.unravel_index(np.argmin(distance), distance.shape)\n",
    "print(f\"Min euclidean distance:\", min_dist, (disease_names[min_idx[0]], disease_names[min_idx[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde3c3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(disease_embeddings, disease_embeddings) #direct from the dimensionality reduction\n",
    "np.fill_diagonal(similarity, 0)\n",
    "max_sim = np.max(similarity)\n",
    "max_idx = np.unravel_index(np.argmax(similarity), similarity.shape)\n",
    "print(f\"Max cosine similarity:\", max_sim, (disease_labels[max_idx[0]], disease_labels[max_idx[1]]))\n",
    "\n",
    "# Compute euclidean distances avoiding self-comparisons\n",
    "distance = euclidean_distances(disease_embeddings, disease_embeddings)\n",
    "np.fill_diagonal(distance, np.inf)\n",
    "min_dist = distance.min()\n",
    "min_idx = np.unravel_index(np.argmin(distance), distance.shape)\n",
    "print(f\"Min euclidean distance:\", min_dist, (disease_labels[min_idx[0]], disease_labels[min_idx[1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10551d91",
   "metadata": {},
   "source": [
    "semantic similarity:\n",
    "1. genenerate embeddings from the 'node_name', these embeddings should capture relationships among the terms grouping similar closer together.\n",
    "2. calculate pairwise cosine similarity/Eucleidian distance\n",
    "    - (both methods agree on the most similar pair, which corresponds to the same pair in the graph)\n",
    "\n",
    "?? should I apply dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fd7b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "comparison of both:\n",
    "-   between the distance from the 'node_name' and the distance from the index\n",
    "- the distance value for the same pair should be similar: large distance for pairs distant in the index and in the 'node_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836cd4a",
   "metadata": {},
   "source": [
    "## Temperature plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114030f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# calculate Pearson correlations among the embeddings in index_nodes\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index_nodes_embeddings = np.array([node.embedding for node in index_nodes])\n",
    "correlations = np.corrcoef(index_nodes_embeddings)\n",
    "\n",
    "#plot half of the correlation matrix with labels and correlation values\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(correlations, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(index_nodes)), [node.metadata['node_name'] for node in index_nodes], rotation=90)\n",
    "plt.yticks(range(len(index_nodes)), [node.metadata['node_name'] for node in index_nodes])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
